"""
Author: XIN LI

TODO:
* Derive a "abstractive definition", which can be automatically generated by model.
* such "abstractive definition" (using [Probability Rules] to represent) should be:

abstractive:
- human: cover all humen, but can not be used to represent a specific human (black, white, yellow)
- color: cover all colors, but can not be used to represent a specific color (green, red, yellow)
- seven, cover all sevens, but can not be used to represent a specific hand-writing "7"

definition:
- human: leg, arm, heads; but all these components are allow to be different and yet similar in some ways




Using playing saxophone as an example.

A particular "KEY" is hard-defined to be a KEY on the saxophone. But every time you play the KEY, it will be 
different (similar, but never the same as last time, due to the air blow).

So, how can i setup a similar hard-defined KEY in my model (instead of on the sax)?







inheritable difference: 


Normalize the scale to the same:
- X => 0 ~ 1
- Y => 0 ~ 1
- X => min ~ max





what's the meaning of "alike"?
- in statistics, we use "distance" to evaluate the "likeness"
- in abstraction, we can try to use "substitutability" to evaluate the "likeness"






when you close your eyes, imagin a pattern, you see it, but actually you didn't,
because it's actually all black when you close your eyes. it is the brain that
read and give you a memory of such pattern to "make you think" you see it.

the amazing part is that, since there is not actual pattern to relfect the light into
eyes to see such pattern, and human can not remember the "pixel values" to remember the pattern,
how we store such pattern in our memory and read it out when we close our eyes?
"""
# ---------------------------------- customized libs
from utils import *
from feel import Feel
from nobo import NOBO
from btb import BTB
# ---------------------------------- open-source libs
from mnist import MNIST
from matplotlib import pyplot as plt
import numpy as np
import random
import math
from PIL import Image
from mpl_toolkits.mplot3d import Axes3D

mndata = MNIST('dataset')
images_train, labels_train = mndata.load_training()
images_test, labels_test = mndata.load_testing()


# ################################################## init
isTraining = True
#isTraining = False

# -------------------------------------------------- train
#isRandom = True
isRandom = False
training_size = 25
size_train = training_size * 10
# -------------------------------------------------- test
testing_size = 50
size_test = testing_size * 10

# TODO: try different weights
# ---------------------- weight of abstraction lvl
w1 = 1
w2 = 3 #3
w3 = 5 #5
# ---------------------- 


correctness = 0
local_correctness = [0] * 10

# -------------------------------------------------- save
path_to_file1 = "_saved/save_spectial_10_lvl1"
path_to_file2 = "_saved/save_spectial_10_lvl2"
path_to_file3 = "_saved/save_spectial_10_lvl3"
"""
path_to_file1 = "_saved/save_" + str(training_size) + "_lvl1"
path_to_file2 = "_saved/save_" + str(training_size) + "_lvl2"
path_to_file3 = "_saved/save_" + str(training_size) + "_lvl3"
"""
# ################################################## init


if isTraining:
	enc_tracker = [0] * 10

	model1 = np.zeros((10, 16, 16), dtype = np.float)
	model2 = np.zeros((10, 16, 16, 16), dtype = np.float)
	model3 = np.zeros((10, 16, 16, 16, 16), dtype = np.float)
	

	


	"""
	imgs, labs = get_data(size_train, isTraining, isRandom)
	"""
	imgs, labs = get_special_data()






	ep = len(imgs)

	for i in range(ep):
		print("Training ... ... ... ... " + str(ep) + " - " + str(i))

		img = imgs[i]
		lab = labs[i]

		btb = BTB(2, img)
		btb.perception()

		model1[lab, :, :] = model1[lab, :, :] + btb.lvl1
		model2[lab, :, :, :] = model2[lab, :, :, :] + btb.lvl2
		model3[lab, :, :, :, :] = model3[lab, :, :, :, :] + btb.lvl3

		enc_tracker[lab] = enc_tracker[lab] + 1

	for i in range(10):
		norm = float(enc_tracker[i])
		model1[i, :, :] = model1[i, :, :] / norm
		model2[i, :, :, :] = model2[i, :, :, :] / norm
		model3[i, :, :, :, :] = model3[i, :, :, :, :] / norm

	print("#####################")
	np.save(path_to_file1, model1)
	np.save(path_to_file2, model2)
	np.save(path_to_file3, model3)
	print("... models are saved!")
	print("#####################")

else:
	m1 = np.load(path_to_file1 + ".npy")
	m2 = np.load(path_to_file2 + ".npy")
	m3 = np.load(path_to_file3 + ".npy")


	enc_tracker = [0] * 10
	imgs, labs = get_data(size_test, isTraining, isRandom)
	ep = len(imgs)

	for i in range(ep):
		print("Testing ... ... ... ... " + str(ep) + " - " + str(i))

		res = []
		res1 = []
		res2 = []
		res3 = []

		img = imgs[i]
		lab = labs[i]
		enc_tracker[lab] = enc_tracker[lab] + 1

		btb = BTB(2, img)
		btb.perception()

		r1 = btb.lvl1
		r2 = btb.lvl2
		r3 = btb.lvl3

		for j in range(10):
			prod1 = np.multiply(m1[j], r1)
			prod2 = np.multiply(m2[j], r2)
			prod3 = np.multiply(m3[j], r3)

			ratio1 = prod1.sum() / m1[j].sum()
			ratio2 = prod2.sum() / m2[j].sum()
			ratio3 = prod3.sum() / m3[j].sum()

			res1.append(ratio1)
			res2.append(ratio2)
			res3.append(ratio3)

		reorder1 = sorted(range(len(res1)), key=lambda k: res1[k], reverse=True)
		reorder2 = sorted(range(len(res2)), key=lambda k: res2[k], reverse=True)
		reorder3 = sorted(range(len(res3)), key=lambda k: res3[k], reverse=True)

		for c in range(10):
			rank_l1 = float(reorder1.index(c)) * w1
			rank_l2 = float(reorder2.index(c)) * w2
			rank_l3 = float(reorder3.index(c)) * w3
			res.append(rank_l1 + rank_l2 + rank_l3)

		# --------------------------------------------- assess (1): exact
		"""
		pred = res.index(min(res))

		print(str(lab) + " - " + str(pred))
		print(sorted(range(len(res)), key=lambda k: res[k]))

		if lab == pred:
			correctness = correctness + 1
			local_correctness[lab] = local_correctness[lab] + 1

		"""
		# --------------------------------------------- assess (2): in
		res_min = min(res)

		pred = [i for i, x in enumerate(res) if x == res_min]
		print(str(lab) + " - " + str(pred))
		print(sorted(range(len(res)), key=lambda k: res[k]))

		if lab in pred:
			correctness = correctness + 1
			local_correctness[lab] = local_correctness[lab] + 1

	# ---------------------------------------- prep summary
	for i in range(len(local_correctness)):
		local_correctness[i] = float(local_correctness[i]) / float(enc_tracker[i])

	sum_correctness = float(correctness) / float(ep)
	
	print(" ################ Summary ################")
	print(local_correctness)
	print(sum_correctness)
	print("##########################################")


"""
# --------------------------------------------------------
l1 & l2 & l3 + 25:

[0.72, 0.9, 0.32, 0.8, 0.82, 0.58, 0.64, 0.68, 0.46, 0.54]
0.646

l1 & l2 & l3 + 1:
[0.46, 0.7, 0.24, 0.1, 0.52, 0.32, 0.08, 0.38, 0.02, 0.24]
0.306
# --------------------------------------------------------
"""



"""
----------- 5
# 1 3 5
[0.78, 0.92, 0.48, 0.94, 0.5, 0.52, 0.54, 0.76, 0.38, 0.6]
0.642

# 1 5 7
[0.76, 0.88, 0.46, 0.84, 0.5, 0.5, 0.66, 0.72, 0.46, 0.5]
0.628

# 5 3 1
[0.58, 0.92, 0.24, 0.48, 0.12, 0.36, 0.24, 0.62, 0.2, 0.28]
0.404

----------- 25
# 1 3 5
[0.86, 0.96, 0.74, 0.8, 0.82, 0.66, 0.62, 0.84, 0.84, 0.62]
0.776


----------- 10 (special)
# 1 3 5
[0.82, 0.86, 0.6, 0.58, 0.46, 0.7, 0.36, 0.7, 0.58, 0.36]
0.602

# 1 2 3
[0.68, 0.8, 0.52, 0.7, 0.48, 0.7, 0.56, 0.66, 0.6, 0.5]
0.62

# 1 2 3 with remove diffs











"""













